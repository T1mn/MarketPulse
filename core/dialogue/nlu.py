"""NLU å¼•æ“ - è‡ªç„¶è¯­è¨€ç†è§£"""
import json
import logging
from typing import Dict, List, Optional
from dataclasses import dataclass

from config import chatbot_config
from core.llm.router import llm_router
from core.llm.prompt_manager import prompt_manager

logger = logging.getLogger(__name__)


@dataclass
class Intent:
    """æ„å›¾è¯†åˆ«ç»“æœ"""
    name: str
    confidence: float
    reasoning: str
    entities: Dict[str, any]


@dataclass
class Entity:
    """å®ä½“æå–ç»“æœ"""
    type: str
    value: str
    confidence: float
    start: Optional[int] = None
    end: Optional[int] = None


@dataclass
class NLUResult:
    """NLU å®Œæ•´ç»“æœ"""
    intent: Intent
    entities: List[Entity]
    language: str
    confidence: float


class NLUEngine:
    """
    è‡ªç„¶è¯­è¨€ç†è§£å¼•æ“

    åŠŸèƒ½ï¼š
    1. æ„å›¾è¯†åˆ«ï¼ˆIntent Recognitionï¼‰
    2. å®ä½“æå–ï¼ˆEntity Extractionï¼‰
    3. è¯­è¨€æ£€æµ‹ï¼ˆLanguage Detectionï¼‰
    4. æƒ…æ„Ÿåˆ†æï¼ˆSentiment Analysisï¼‰
    """

    def __init__(self):
        self.intents = chatbot_config.intents
        self.entity_types = chatbot_config.entity_types
        self.confidence_threshold = chatbot_config.intent_confidence_threshold

    async def understand(
        self,
        user_input: str,
        context: Optional[List[Dict]] = None,
        language: Optional[str] = None
    ) -> NLUResult:
        """
        ç†è§£ç”¨æˆ·è¾“å…¥

        Args:
            user_input: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
            context: å¯¹è¯ä¸Šä¸‹æ–‡
            language: è¯­è¨€ï¼ˆè‡ªåŠ¨æ£€æµ‹å¦‚æœæœªæä¾›ï¼‰

        Returns:
            NLUResult: NLU ç»“æœ
        """
        logger.info(f"ğŸ” NLU analyzing: {user_input[:50]}...")

        # 1. è¯­è¨€æ£€æµ‹
        if not language:
            language = self._detect_language(user_input)

        # 2. æ„å›¾è¯†åˆ«
        intent = await self._classify_intent(user_input, context or [], language)

        # 3. å®ä½“æå–
        entities = await self._extract_entities(user_input, intent.name, language)

        # 4. ç»¼åˆç½®ä¿¡åº¦
        overall_confidence = self._calculate_confidence(intent, entities)

        result = NLUResult(
            intent=intent,
            entities=entities,
            language=language,
            confidence=overall_confidence
        )

        logger.info(
            f"âœ… NLU result: intent={intent.name} ({intent.confidence:.2f}), "
            f"entities={len(entities)}, language={language}"
        )

        return result

    async def _classify_intent(
        self,
        user_input: str,
        context: List[Dict],
        language: str
    ) -> Intent:
        """æ„å›¾åˆ†ç±»"""

        # æ„å»º Prompt
        prompt = prompt_manager.get_nlu_intent_prompt(
            user_input=user_input,
            intents=self.intents,
            context=context,
            language=language
        )

        # è°ƒç”¨ LLMï¼ˆä½¿ç”¨å¿«é€Ÿæ¨¡å‹ï¼‰
        try:
            response = await llm_router.generate(
                prompt=prompt,
                model_preference="speed",  # æ„å›¾è¯†åˆ«ç”¨å¿«é€Ÿæ¨¡å‹
                temperature=0.1,  # ä½æ¸©åº¦ä¿è¯ç¨³å®šæ€§
                max_tokens=500,
            )

            # è§£æå“åº”
            result = self._parse_json_response(response.content)

            return Intent(
                name=result.get("intent", "unknown"),
                confidence=result.get("confidence", 0.0),
                reasoning=result.get("reasoning", ""),
                entities=result.get("entities", {})
            )

        except Exception as e:
            logger.error(f"Intent classification failed: {e}")
            # é™çº§ï¼šè¿”å›é»˜è®¤æ„å›¾
            return Intent(
                name="customer_service",
                confidence=0.5,
                reasoning="Fallback to default intent due to error",
                entities={}
            )

    async def _extract_entities(
        self,
        text: str,
        intent: str,
        language: str
    ) -> List[Entity]:
        """å®ä½“æå–"""

        # è·å–è¯¥æ„å›¾éœ€è¦çš„å®ä½“ç±»å‹
        intent_config = self.intents.get(intent)
        if not intent_config or not intent_config.required_entities:
            return []

        # æ„å»º Prompt
        prompt = prompt_manager.get_entity_extraction_prompt(
            text=text,
            entity_types=self.entity_types,
            language=language
        )

        try:
            response = await llm_router.generate(
                prompt=prompt,
                model_preference="speed",
                temperature=0.1,
                max_tokens=500,
            )

            # è§£æå“åº”
            result = self._parse_json_response(response.content)
            entities_data = result.get("entities", [])

            entities = []
            for entity_data in entities_data:
                entity = Entity(
                    type=entity_data.get("type", "unknown"),
                    value=entity_data.get("value", ""),
                    confidence=entity_data.get("confidence", 0.0),
                )
                entities.append(entity)

            return entities

        except Exception as e:
            logger.error(f"Entity extraction failed: {e}")
            return []

    def _detect_language(self, text: str) -> str:
        """
        è¯­è¨€æ£€æµ‹

        ç®€å•å¯å‘å¼è§„åˆ™ï¼š
        - åŒ…å«ä¸­æ–‡å­—ç¬¦ â†’ zh-CN
        - å¦åˆ™ â†’ en
        """
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        total_chars = len(text)

        if chinese_chars / max(total_chars, 1) > 0.3:
            return "zh-CN"
        else:
            return "en"

    def _calculate_confidence(self, intent: Intent, entities: List[Entity]) -> float:
        """è®¡ç®—ç»¼åˆç½®ä¿¡åº¦"""
        # åŸºç¡€ç½®ä¿¡åº¦æ¥è‡ªæ„å›¾
        base_confidence = intent.confidence

        # å¦‚æœæ‰¾åˆ°äº†æ‰€éœ€çš„å®ä½“ï¼Œæå‡ç½®ä¿¡åº¦
        intent_config = self.intents.get(intent.name)
        if intent_config and intent_config.required_entities:
            required_count = len(intent_config.required_entities)
            found_count = len(entities)
            entity_bonus = min(found_count / max(required_count, 1), 1.0) * 0.1
            base_confidence += entity_bonus

        return min(base_confidence, 1.0)

    def _parse_json_response(self, response: str) -> Dict:
        """è§£æ LLM çš„ JSON å“åº”"""
        try:
            # æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—
            clean_response = response.strip()
            if clean_response.startswith("```json"):
                clean_response = clean_response[7:]
            if clean_response.startswith("```"):
                clean_response = clean_response[3:]
            if clean_response.endswith("```"):
                clean_response = clean_response[:-3]

            return json.loads(clean_response.strip())

        except json.JSONDecodeError as e:
            logger.error(f"JSON parse error: {e}, response: {response[:200]}")
            return {}
