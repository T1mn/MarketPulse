"""LLM æ™ºèƒ½è·¯ç”±å™¨"""
from typing import List, Optional, Literal, Dict
import asyncio
import logging

from config import settings, llm_config
from .base import BaseLLM, LLMMessage, LLMResponse, LLMError
from .providers.deepseek import DeepSeekLLM
from .providers.gemini import GeminiLLM
from .providers.openai import OpenAILLM
from .cache import LLMCache

logger = logging.getLogger(__name__)


class LLMRouter:
    """
    LLM æ™ºèƒ½è·¯ç”±å™¨

    åŠŸèƒ½ï¼š
    1. å¤šæ¨¡å‹ç®¡ç†ï¼ˆDeepSeek, Gemini, OpenAIï¼‰
    2. æ™ºèƒ½è·¯ç”±ï¼ˆåŸºäºæˆæœ¬/é€Ÿåº¦/è´¨é‡ï¼‰
    3. è‡ªåŠ¨é™çº§ï¼ˆä¸»æ¨¡å‹å¤±è´¥æ—¶åˆ‡æ¢å¤‡ç”¨ï¼‰
    4. æˆæœ¬è¿½è¸ª
    5. ç¼“å­˜ç®¡ç†
    """

    def __init__(self):
        self.cache = LLMCache()
        self.providers: Dict[str, BaseLLM] = {}
        self._init_providers()

        # æˆæœ¬è¿½è¸ª
        self.total_cost = 0.0
        self.request_count = 0

    def _init_providers(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„ LLM æä¾›å•†"""

        # DeepSeek
        if settings.DEEPSEEK_API_KEY:
            try:
                self.providers["deepseek"] = DeepSeekLLM(
                    api_key=settings.DEEPSEEK_API_KEY,
                    model_name="deepseek-chat",
                )
                logger.info("âœ… DeepSeek provider initialized")
            except Exception as e:
                logger.warning(f"âŒ Failed to init DeepSeek: {e}")

        # Gemini
        if settings.GEMINI_API_KEY:
            try:
                self.providers["gemini"] = GeminiLLM(
                    api_key=settings.GEMINI_API_KEY,
                    model_name="gemini-2.0-flash-exp",
                )
                logger.info("âœ… Gemini provider initialized")
            except Exception as e:
                logger.warning(f"âŒ Failed to init Gemini: {e}")

        # OpenAI
        if settings.OPENAI_API_KEY:
            try:
                self.providers["openai"] = OpenAILLM(
                    api_key=settings.OPENAI_API_KEY,
                    model_name="gpt-4o-mini",
                )
                logger.info("âœ… OpenAI provider initialized")
            except Exception as e:
                logger.warning(f"âŒ Failed to init OpenAI: {e}")

        if not self.providers:
            raise ValueError("No LLM providers available! Please configure API keys.")

    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        model_preference: Optional[Literal["cost", "speed", "quality", "balanced"]] = None,
        temperature: float = 0.7,
        max_tokens: int = 4096,
        use_cache: bool = True,
        **kwargs
    ) -> LLMResponse:
        """
        æ™ºèƒ½ç”Ÿæˆæ–‡æœ¬

        Args:
            prompt: ç”¨æˆ·è¾“å…¥
            system_prompt: ç³»ç»Ÿæç¤º
            model_preference: æ¨¡å‹åå¥½
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            LLMResponse: å“åº”å¯¹è±¡
        """
        # 1. æ„å»ºæ¶ˆæ¯
        messages = []
        if system_prompt:
            messages.append(LLMMessage(role="system", content=system_prompt))
        messages.append(LLMMessage(role="user", content=prompt))

        # 2. æ£€æŸ¥ç¼“å­˜
        if use_cache and llm_config.enable_cache:
            cache_key = self.cache.generate_key(messages, temperature, max_tokens)
            cached_response = self.cache.get(cache_key)
            if cached_response:
                logger.info("âœ… Cache hit")
                cached_response.cached = True
                return cached_response

        # 3. é€‰æ‹©æœ€ä½³æ¨¡å‹
        provider = self._select_provider(model_preference or llm_config.routing_strategy)

        # 4. è°ƒç”¨æ¨¡å‹ï¼ˆå¸¦é‡è¯•å’Œé™çº§ï¼‰
        response = await self._generate_with_fallback(
            provider=provider,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )

        # 5. ç¼“å­˜å“åº”
        if use_cache and llm_config.enable_cache:
            self.cache.set(cache_key, response, ttl=llm_config.cache_ttl)

        # 6. è®°å½•æˆæœ¬
        self._track_cost(response)

        return response

    def _select_provider(
        self,
        strategy: Literal["cost", "speed", "quality", "balanced"]
    ) -> str:
        """
        æ ¹æ®ç­–ç•¥é€‰æ‹©æœ€ä½³æä¾›å•†

        ç­–ç•¥è¯´æ˜ï¼š
        - cost: æˆæœ¬æœ€ä½ï¼ˆä¼˜å…ˆ DeepSeekï¼‰
        - speed: é€Ÿåº¦æœ€å¿«ï¼ˆä¼˜å…ˆ DeepSeekï¼‰
        - quality: è´¨é‡æœ€é«˜ï¼ˆä¼˜å…ˆ Geminiï¼‰
        - balanced: ç»¼åˆå¹³è¡¡ï¼ˆDeepSeek æ€§ä»·æ¯”é«˜ï¼‰
        """
        if strategy == "cost":
            # DeepSeek æœ€ä¾¿å®œ
            return "deepseek" if "deepseek" in self.providers else list(self.providers.keys())[0]

        elif strategy == "speed":
            # DeepSeek é€Ÿåº¦å¿«
            return "deepseek" if "deepseek" in self.providers else list(self.providers.keys())[0]

        elif strategy == "quality":
            # Gemini è´¨é‡é«˜ï¼ˆä½†ä¹Ÿå¯ä»¥ç”¨ DeepSeek Reasonerï¼‰
            return "gemini" if "gemini" in self.providers else list(self.providers.keys())[0]

        else:  # balanced
            # DeepSeek æ€§ä»·æ¯”æœ€é«˜
            return "deepseek" if "deepseek" in self.providers else list(self.providers.keys())[0]

    async def _generate_with_fallback(
        self,
        provider: str,
        messages: List[LLMMessage],
        temperature: float,
        max_tokens: int,
        **kwargs
    ) -> LLMResponse:
        """å¸¦é™çº§ç­–ç•¥çš„ç”Ÿæˆ"""

        # æ„å»ºå°è¯•é¡ºåº
        if llm_config.enable_fallback:
            try_order = [provider] + [
                p for p in llm_config.fallback_order
                if p != provider and p in self.providers
            ]
        else:
            try_order = [provider]

        last_error = None

        for attempt, provider_name in enumerate(try_order):
            try:
                logger.info(f"ğŸ”„ Attempt {attempt + 1}: Using {provider_name}")

                llm = self.providers[provider_name]
                response = await llm.generate(
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    **kwargs
                )

                if attempt > 0:
                    logger.warning(f"âœ… Fallback successful: {provider} â†’ {provider_name}")

                return response

            except Exception as e:
                last_error = e
                logger.error(f"âŒ {provider_name} failed: {str(e)}")

                if attempt < len(try_order) - 1:
                    # è¿˜æœ‰å¤‡é€‰ï¼Œç»§ç»­é‡è¯•
                    await asyncio.sleep(llm_config.retry_delay)
                    continue
                else:
                    # æ‰€æœ‰å¤‡é€‰éƒ½å¤±è´¥
                    break

        # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥
        raise LLMError(f"All providers failed. Last error: {last_error}")

    def _track_cost(self, response: LLMResponse):
        """è¿½è¸ª API æˆæœ¬"""
        model_config = llm_config.models.get(response.model)
        if model_config:
            cost = (response.usage["total_tokens"] / 1000) * model_config.cost_per_1k_tokens
            self.total_cost += cost
            self.request_count += 1

            logger.info(
                f"ğŸ’° Cost tracking: ${cost:.6f} | "
                f"Total: ${self.total_cost:.4f} | "
                f"Requests: {self.request_count}"
            )

    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_cost": self.total_cost,
            "request_count": self.request_count,
            "avg_cost_per_request": self.total_cost / self.request_count if self.request_count > 0 else 0,
            "available_providers": list(self.providers.keys()),
            "cache_stats": self.cache.get_stats(),
        }


# å…¨å±€è·¯ç”±å™¨å®ä¾‹
llm_router = LLMRouter()
