"""RAG æ£€ç´¢å™¨"""
import logging
from typing import List, Dict, Optional, Any
from dataclasses import dataclass

from .vector_store import VectorStore, vector_store, Document
from .embeddings import embedding_model
from config import chatbot_config

logger = logging.getLogger(__name__)


@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    content: str
    metadata: Dict[str, Any]
    score: float  # ç›¸ä¼¼åº¦åˆ†æ•°
    source: str


class RAGRetriever:
    """
    RAG æ£€ç´¢å™¨

    åŠŸèƒ½ï¼š
    1. å‘é‡æ£€ç´¢
    2. å…³é”®è¯æ£€ç´¢ï¼ˆTODOï¼‰
    3. æ··åˆæ£€ç´¢ï¼ˆTODOï¼‰
    4. é‡æ’åºï¼ˆTODOï¼‰
    """

    def __init__(
        self,
        vector_store: VectorStore = None,
        top_k: int = 5,
        similarity_threshold: float = 0.7
    ):
        self.vector_store = vector_store or globals()['vector_store']
        self.top_k = top_k
        self.similarity_threshold = similarity_threshold

    async def retrieve(
        self,
        query: str,
        filters: Optional[Dict] = None,
        top_k: Optional[int] = None
    ) -> List[RetrievalResult]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            filters: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶
            top_k: è¿”å›ç»“æœæ•°é‡

        Returns:
            List[RetrievalResult]: æ£€ç´¢ç»“æœ
        """
        k = top_k or self.top_k

        # 1. å‘é‡æ£€ç´¢
        docs = self.vector_store.search(
            query=query,
            top_k=k * 2,  # å¤šæ£€ç´¢ä¸€äº›ï¼Œç”¨äºåç»­è¿‡æ»¤
            filter=filters
        )

        # 2. è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
        results = []
        for doc in docs:
            # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆè·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼‰
            # ChromaDB ä½¿ç”¨ L2 è·ç¦»ï¼Œè½¬æ¢ä¸ºç›¸ä¼¼åº¦
            distance = doc.get('distance', 0)
            similarity = 1 / (1 + distance)  # ç®€å•è½¬æ¢

            if similarity >= self.similarity_threshold:
                result = RetrievalResult(
                    content=doc['content'],
                    metadata=doc.get('metadata', {}),
                    score=similarity,
                    source=doc.get('metadata', {}).get('source', 'unknown')
                )
                results.append(result)

        # 3. æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶é™åˆ¶æ•°é‡
        results.sort(key=lambda x: x.score, reverse=True)
        results = results[:k]

        logger.info(f"ğŸ” Retrieved {len(results)} documents (threshold={self.similarity_threshold})")

        return results

    async def build_context(
        self,
        query: str,
        intent: str,
        max_length: int = 2000
    ) -> str:
        """
        æ„å»º RAG ä¸Šä¸‹æ–‡

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            intent: æ„å›¾
            max_length: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰

        Returns:
            str: æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡
        """
        # æ ¹æ®æ„å›¾è®¾ç½®è¿‡æ»¤æ¡ä»¶
        filters = self._get_filters_by_intent(intent)

        # æ£€ç´¢æ–‡æ¡£
        results = await self.retrieve(query, filters=filters)

        if not results:
            return ""

        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        current_length = 0

        for i, result in enumerate(results, 1):
            # æ ¼å¼åŒ–å•ä¸ªæ–‡æ¡£
            doc_text = f"ã€æ–‡æ¡£ {i}ã€‘\næ¥æºï¼š{result.source}\nå†…å®¹ï¼š{result.content}\n"

            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            if current_length + len(doc_text) > max_length:
                break

            context_parts.append(doc_text)
            current_length += len(doc_text)

        context = "\n".join(context_parts)

        logger.info(f"ğŸ“ Built context: {len(context)} chars from {len(context_parts)} docs")

        return context

    def _get_filters_by_intent(self, intent: str) -> Optional[Dict]:
        """æ ¹æ®æ„å›¾è·å–è¿‡æ»¤æ¡ä»¶"""
        # ä¸åŒæ„å›¾å¯èƒ½éœ€è¦ä¸åŒç±»å‹çš„çŸ¥è¯†
        intent_filters = {
            "market_query": {"category": "market_data"},
            "news_analysis": {"category": "news"},
            "customer_service": {"category": "faq"},
            "trading": {"category": "trading_rules"},
        }

        return intent_filters.get(intent)

    async def add_knowledge(
        self,
        content: str,
        metadata: Dict[str, Any]
    ) -> bool:
        """
        æ·»åŠ çŸ¥è¯†åˆ°çŸ¥è¯†åº“

        Args:
            content: æ–‡æ¡£å†…å®¹
            metadata: å…ƒæ•°æ®ï¼ˆcategory, source, titleç­‰ï¼‰

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        # ç”Ÿæˆæ–‡æ¡£ ID
        import hashlib
        doc_id = hashlib.md5(content.encode()).hexdigest()

        # åˆ›å»ºæ–‡æ¡£
        doc = Document(
            id=doc_id,
            content=content,
            metadata=metadata
        )

        # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
        success = self.vector_store.add_documents([doc])

        if success:
            logger.info(f"âœ… Added knowledge: {metadata.get('title', doc_id)}")

        return success

    async def batch_add_knowledge(
        self,
        documents: List[Dict[str, Any]]
    ) -> int:
        """
        æ‰¹é‡æ·»åŠ çŸ¥è¯†

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« content å’Œ metadata

        Returns:
            int: æˆåŠŸæ·»åŠ çš„æ•°é‡
        """
        docs = []
        for doc_data in documents:
            import hashlib
            content = doc_data['content']
            metadata = doc_data.get('metadata', {})

            doc_id = hashlib.md5(content.encode()).hexdigest()

            doc = Document(
                id=doc_id,
                content=content,
                metadata=metadata
            )
            docs.append(doc)

        success = self.vector_store.add_documents(docs)

        if success:
            logger.info(f"âœ… Batch added {len(docs)} documents")
            return len(docs)

        return 0


# å…¨å±€æ£€ç´¢å™¨å®ä¾‹
retriever = RAGRetriever()
